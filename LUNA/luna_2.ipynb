{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jungeun202/Transformer/blob/main/luna_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LUNA transformer with smaller dataset"
      ],
      "metadata": {
        "id": "iHqEdo9bpsFE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUgohN2uxJXJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import math\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding"
      ],
      "metadata": {
        "id": "eBDR3JRJxPJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "  def __init__(self, vocab_num, d_model):\n",
        "    super(Embeddings,self).__init__()\n",
        "    self.emb = nn.Embedding(vocab_num,d_model)\n",
        "    self.d_model = d_model\n",
        "  def forward(self, x):\n",
        "\n",
        "    return self.emb(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "KPi2UfYPxOGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int = 80, max_length: int = 5000) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_length, d_model, requires_grad=False) # initialized to store the positional encodings | no need to compute gradients\n",
        "        # generate a 1d tensor containing values 0 to max_length - 1 in floating-point format | adds an extra dimension to the tensor ; [max_length] -> [max_length, 1]\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1).float()\n",
        "        # creates a sequence of even numbers from 0 to less than max_length | a negative scaling factor and normalize by dividing d_model | exponentiates the scaled indices\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # for each even-indexed dimension\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # for each odd-indexed dimension\n",
        "        # position tensor shape is [max_length, 1] and div_term has shape [d_model/2] ; scaled by each div_term\n",
        "        # pe has a shape of [max_length, d_model]\n",
        "        pe = pe.unsqueeze(0) # add an extra dimension ; [max_length, d_model, 1]\n",
        "        self.register_buffer('pe', pe) # save as a buffer; easily available during model training and inference without recomputation\n",
        "\n",
        "    def forward(self, length: int) -> Tensor:\n",
        "        return self.pe[:, :length]"
      ],
      "metadata": {
        "id": "xpwQWiyixagd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention\n",
        "*   (LUNA paper) dot product attention\n",
        "*   (transformer) self attention\n",
        "\n"
      ],
      "metadata": {
        "id": "6CfcRaCQxbbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    r\"\"\"\n",
        "    Scaled Dot-Product Attention proposed in \"Attention Is All You Need\"\n",
        "    Compute the dot products of the query with all keys, divide each by sqrt(dim),\n",
        "    and apply a softmax function to obtain the weights on the values\n",
        "\n",
        "    Args: dim, mask\n",
        "        dim (int): dimension of attention\n",
        "        mask (torch.Tensor): tensor containing indices to be masked\n",
        "\n",
        "    Inputs: query, key, value, mask\n",
        "        - **query** (batch, q_len, d_model): tensor containing projection vector for decoders.\n",
        "        - **key** (batch, k_len, d_model): tensor containing projection vector for encoders.\n",
        "        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
        "        - **mask** (-): tensor containing indices to be masked\n",
        "\n",
        "    Returns: context, attn\n",
        "        - **context**: tensor containing the context vector from attention mechanism.\n",
        "        - **attn**: tensor containing the attention (alignment) from the encoders outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, scale: bool = True) -> None:\n",
        "        super(DotProductAttention, self).__init__()\n",
        "        if scale:\n",
        "            self.sqrt_dim = np.sqrt(dim)\n",
        "        else:\n",
        "            self.sqrt_dim = 1\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query: torch.FloatTensor,\n",
        "            key: torch.FloatTensor,\n",
        "            value: torch.FloatTensor,\n",
        "            mask: Optional[torch.FloatTensor] = None,\n",
        "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
        "        score = torch.matmul(query, key.transpose(2, 3)) / self.sqrt_dim\n",
        "\n",
        "        if mask is not None:\n",
        "            score.masked_fill_(mask, -1e4)\n",
        "\n",
        "        attn = F.softmax(score, -1)\n",
        "\n",
        "        if len(query.size()) == 3:\n",
        "            context = torch.bmm(attn, value)\n",
        "        else:\n",
        "            context = torch.matmul(attn, value)\n",
        "\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    r\"\"\"\n",
        "    Multi-Head Attention proposed in \"Attention Is All You Need\"\n",
        "    Instead of performing a single attention function with d_model-dimensional keys, values, and queries,\n",
        "    project the queries, keys and values h times with different, learned linear projections to d_head dimensions.\n",
        "    These are concatenated and once again projected, resulting in the final values.\n",
        "    Multi-head attention allows the model to jointly attend to information from different representation\n",
        "    subspaces at different positions.\n",
        "\n",
        "    MultiHead(Q, K, V) = Concat(head_1, ..., head_h) 路 W_o\n",
        "        where head_i = Attention(Q 路 W_q, K 路 W_k, V 路 W_v)\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of model (default: 512)\n",
        "        num_attention_heads (int): The number of attention heads. (default: 8)\n",
        "\n",
        "    Inputs: query, key, value, mask\n",
        "        - **query** (batch, q_len, d_model): tensor containing projection vector for decoders.\n",
        "        - **key** (batch, k_len, d_model): tensor containing projection vector for encoders.\n",
        "        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
        "        - **mask** (-): tensor containing indices to be masked\n",
        "\n",
        "    Returns: output, attn\n",
        "        - **output** (batch, output_len, dimensions): tensor containing the attended output features.\n",
        "        - **attn** (batch * num_attention_heads, v_len): tensor containing the attention (alignment) from the encoders outputs.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int = 512, num_attention_heads: int = 8) -> None:\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        assert dim % num_attention_heads == 0, \"hidden_dim % num_attention_heads should be zero.\"\n",
        "\n",
        "        self.d_head = int(dim / num_attention_heads)\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.query_proj = nn.Linear(dim, self.d_head * num_attention_heads)\n",
        "        self.key_proj = nn.Linear(dim, self.d_head * num_attention_heads)\n",
        "        self.value_proj = nn.Linear(dim, self.d_head * num_attention_heads)\n",
        "        self.scaled_dot_attn = DotProductAttention(dim, scale=True)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query: torch.FloatTensor,\n",
        "            key: torch.FloatTensor,\n",
        "            value: torch.FloatTensor,\n",
        "            mask: Optional[torch.FloatTensor] = None,\n",
        "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
        "        batch_size = value.size(0)\n",
        "\n",
        "        query = self.query_proj(query).view(batch_size, -1, self.num_attention_heads, self.d_head).transpose(1, 2)\n",
        "        key = self.key_proj(key).view(batch_size, -1, self.num_attention_heads, self.d_head).transpose(1, 2)\n",
        "        value = self.value_proj(value).view(batch_size, -1, self.num_attention_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).repeat(1, self.num_attention_heads, 1, 1)\n",
        "\n",
        "        context, attn = self.scaled_dot_attn(query, key, value, mask)\n",
        "\n",
        "        context = context.transpose(1, 2).reshape(batch_size, -1, self.num_attention_heads * self.d_head)\n",
        "\n",
        "        return context, attn\n",
        "\n",
        "\n",
        "class LinearUnifiedNestedAttention(nn.Module): # LUNA has two nested attention operations with an extra input p\n",
        "    def __init__(self, dim, num_attention_heads: int = 8) -> None:\n",
        "        super(LinearUnifiedNestedAttention, self).__init__()\n",
        "        self.pack_attention = MultiHeadAttention(dim, num_attention_heads)\n",
        "        self.unpack_attention = MultiHeadAttention(dim, num_attention_heads)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            query: torch.FloatTensor,\n",
        "            key: torch.FloatTensor,\n",
        "            value: torch.FloatTensor,\n",
        "            p: torch.FloatTensor,\n",
        "            attention_padding_mask: torch.BoolTensor = None,\n",
        "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
        "        packed_context, _ = self.pack_attention(p, key, value, attention_padding_mask) # Pack attention with an extra input p and context sequence\n",
        "        unpacked_context, _ = self.unpack_attention(query, packed_context, packed_context) # Unpack attention with a query and packed context\n",
        "        return unpacked_context, packed_context"
      ],
      "metadata": {
        "id": "Z5LCZZhex0wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feedforward"
      ],
      "metadata": {
        "id": "RjN-JPitx3hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForwardNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feedforward Networks proposed in \"Attention Is All You Need\".\n",
        "    Fully connected feed-forward network, which is applied to each position separately and identically.\n",
        "    This consists of two linear transformations with a ReLU activation in between.\n",
        "    Another way of describing this is as two convolutions with kernel size 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout_p: float = 0.3) -> None:\n",
        "        super(PositionwiseFeedForwardNetwork, self).__init__()\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.Dropout(dropout_p),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout_p),\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        output = self.feed_forward(inputs)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "uc_4xiR7x45m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "fmgNqgYXx9rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LunaTransformerEncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int = 512,\n",
        "            num_attention_heads: int = 8,\n",
        "            d_ff: int = 2048,\n",
        "            dropout_p: float = 0.3,\n",
        "    ) -> None:\n",
        "        super(LunaTransformerEncoderLayer, self).__init__()\n",
        "        self.luna_attention = LinearUnifiedNestedAttention(d_model, num_attention_heads)\n",
        "        self.feed_forward = PositionwiseFeedForwardNetwork(d_model, d_ff, dropout_p)\n",
        "        self.packed_context_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.unpacked_context_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.unpacked_context_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.feed_forward_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            inputs: torch.FloatTensor,\n",
        "            p: torch.FloatTensor,\n",
        "            attention_padding_mask: torch.FloatTensor = None,\n",
        "    ):\n",
        "        unpacked_context, packed_context = self.luna_attention(\n",
        "            query=inputs,\n",
        "            key=inputs,\n",
        "            value=inputs,\n",
        "            p=p,\n",
        "            attention_padding_mask=attention_padding_mask,\n",
        "        )\n",
        "\n",
        "        packed_context = self.packed_context_layer_norm(packed_context + p) # extra input p | pack attention costs O(lm)\n",
        "        unpacked_context = self.unpacked_context_layer_norm(unpacked_context + inputs) # unpack the original query\n",
        "\n",
        "        outputs = self.feed_forward(unpacked_context)\n",
        "        outputs = self.feed_forward_layer_norm(outputs + unpacked_context)\n",
        "\n",
        "        return outputs, packed_context"
      ],
      "metadata": {
        "id": "npSSWyGDyAqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mask"
      ],
      "metadata": {
        "id": "zb-8kkJf6cXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn_pad_mask(inputs, input_lengths, expand_length):\n",
        "    \"\"\" mask position is set to 1 \"\"\"\n",
        "\n",
        "    def get_transformer_non_pad_mask(inputs: Tensor, input_lengths: Tensor) -> Tensor:\n",
        "        \"\"\" Padding position is set to 0, either use input_lengths or pad_id \"\"\"\n",
        "        batch_size = inputs.size(0)\n",
        "\n",
        "        if len(inputs.size()) == 2:\n",
        "            non_pad_mask = inputs.new_ones(inputs.size())  # B x T\n",
        "        elif len(inputs.size()) == 3:\n",
        "            non_pad_mask = inputs.new_ones(inputs.size()[:-1])  # B x T\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported input shape {inputs.size()}\")\n",
        "\n",
        "\n",
        "        for i in range(batch_size):\n",
        "\n",
        "            non_pad_mask[i, int(input_lengths[i]):] = 0\n",
        "\n",
        "        return non_pad_mask\n",
        "\n",
        "    non_pad_mask = get_transformer_non_pad_mask(inputs, input_lengths)\n",
        "    pad_mask = non_pad_mask.lt(1)\n",
        "    attn_pad_mask = pad_mask.unsqueeze(1).expand(-1, expand_length, -1)\n",
        "    return attn_pad_mask\n",
        "\n",
        "\n",
        "def get_attn_subsequent_mask(seq):\n",
        "    assert seq.dim() == 2\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1)\n",
        "\n",
        "    if seq.is_cuda:\n",
        "        subsequent_mask = subsequent_mask.cuda()\n",
        "\n",
        "    return subsequent_mask"
      ],
      "metadata": {
        "id": "EPW8kM8g6dwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "KTZzC96eyGWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LunaTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder architecture applied Linear Unified Nested Attention (Luna).\n",
        "    Luna was proposed in the paper \"Luna: Linear Unified Nested Attention\" (https://arxiv.org/abs/2106.01540.pdf)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size: int,\n",
        "            d_model: int,\n",
        "            num_layers: int = 6,\n",
        "            num_attention_heads: int = 8,\n",
        "            d_ff: int = 2048,\n",
        "            dropout_p: float = 0.1,\n",
        "            project_embedding_length: int = 5,\n",
        "            max_length: int = 1024,\n",
        "    ):\n",
        "        super(LunaTransformerEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.projected_embedding_length = project_embedding_length\n",
        "\n",
        "        self.projected_embeddings = nn.Parameter(torch.Tensor(project_embedding_length, self.d_model))\n",
        "        self.projected_positions = PositionalEncoding(self.d_model, project_embedding_length)\n",
        "        nn.init.normal_(self.projected_embeddings, mean=0.0, std=self.d_model ** -0.5)\n",
        "\n",
        "        self.input_embedding = nn.Embedding(vocab_size, d_model) # vocab_size: 745 , d_model: 4\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "        self.input_positions = PositionalEncoding(d_model, max_length)\n",
        "\n",
        "        self.input_norm = nn.LayerNorm(d_model)\n",
        "        self.embed_scale = math.sqrt(self.d_model)\n",
        "        self.layers = nn.ModuleList([\n",
        "            LunaTransformerEncoderLayer(\n",
        "                d_model=d_model,\n",
        "                num_attention_heads=num_attention_heads,\n",
        "                d_ff=d_ff,\n",
        "                dropout_p=dropout_p,\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.projection_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, input_lengths: torch.Tensor):\n",
        "        batch_size, seq_length = inputs.size()  # batch_size: 20 , seq_length\n",
        "        attention_padding_mask = get_attn_pad_mask(inputs, input_lengths, self.projected_embedding_length)\n",
        "\n",
        "        embedded = self.input_embedding(inputs)\n",
        "        embedded *= self.embed_scale\n",
        "        projected_embedded = self.projected_embeddings * self.embed_scale\n",
        "\n",
        "        embedded += self.input_positions(embedded.size(1))\n",
        "        projected_embedded += self.projected_positions(self.projected_embedding_length).squeeze(0)\n",
        "\n",
        "        seq_length, dim = projected_embedded.size()\n",
        "        projected_embedded = projected_embedded.unsqueeze(0).expand(batch_size, seq_length, dim)\n",
        "\n",
        "        outputs = self.dropout(embedded)\n",
        "        p = self.dropout(projected_embedded)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            outputs, p = layer(outputs, p, attention_padding_mask)\n",
        "\n",
        "\n",
        "        outputs = self.input_norm(outputs)\n",
        "        outputs = self.projection_head(outputs)\n",
        "\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "N7uxYeu0yN8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "tNnpkn8pyOfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "    def get_data(self, path, batch_size=20):\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize the file content\n",
        "        ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        with open(path, 'r') as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "        num_batches = ids.size(0) // batch_size\n",
        "        ids = ids[:num_batches*batch_size]\n",
        "        return ids.view(batch_size, -1)"
      ],
      "metadata": {
        "id": "Tqo6THoayQd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper parameter setting"
      ],
      "metadata": {
        "id": "_AU1G16vyT2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6wzZaK5M1tEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "\n",
        "dim = 128\n",
        "depth = 1\n",
        "num_epochs = 5\n",
        "batch_size = 20\n",
        "sequence_lengths = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "max_seq_len = 50\n",
        "learning_rate = 1e-5\n",
        "head_num = 8\n",
        "times = []\n",
        "\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data('/content/drive/MyDrive/train.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // max_seq_len\n",
        "\n",
        "\n",
        "test_corpus = Corpus()\n",
        "test_ids = test_corpus.get_data('/content/drive/MyDrive/test.txt', batch_size)\n",
        "test_vocab_size =  len(test_corpus.dictionary)\n",
        "test_num_batches = test_ids.size(1) // max_seq_len\n",
        "\n",
        "\n",
        "vocab_size = len(corpus.dictionary)\n",
        "\n",
        "\n",
        "print(num_batches)\n",
        "print(test_num_batches)"
      ],
      "metadata": {
        "id": "TNcI6GSuybFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "xYlxPxTEybhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LunaTransformerEncoder(vocab_size, dim, depth, head_num, max_seq_len, dropout_p=0.1).to(device)"
      ],
      "metadata": {
        "id": "4ue0g2Lf3yiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "for max_seq_len in sequence_lengths:\n",
        "  start_time = time.time()\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "      for i in range(0, ids.size(1) - max_seq_len, max_seq_len):\n",
        "          # Get mini-batch inputs and targets\n",
        "          inputs = ids[:, i:i+max_seq_len].to(device)\n",
        "          targets = ids[:, (i+1):(i+1)+max_seq_len].to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "          # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "\n",
        "          input_lengths = (torch.ones(batch_size) * max_seq_len).to(torch.int32)  # a tensor of the same length for each sequence\n",
        "          outputs = model(inputs, input_lengths)\n",
        "          # process LunaTransformerEncoder's forward method\n",
        "          output_flat = outputs.view(-1, vocab_size) # reshaped\n",
        "\n",
        "          loss = criterion(output_flat, targets.reshape(-1))\n",
        "\n",
        "          # Backward and optimize\n",
        "          model.zero_grad()\n",
        "          loss.backward()\n",
        "          clip_grad_norm_(model.parameters(), 0.5)\n",
        "          optimizer.step()\n",
        "\n",
        "          step = (i+1) // max_seq_len\n",
        "          if step % 500 == 0:\n",
        "              print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                    .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n",
        "\n",
        "\n",
        "  elapsed_time = time.time() - start_time\n",
        "  times.append(elapsed_time)\n",
        "  print('Elapsed time: {:.2f} seconds'.format(elapsed_time))\n",
        "\n",
        "# plt.plot(sequence_lengths, times, marker='*')\n",
        "# plt.xlabel('Sequence Length')\n",
        "# plt.ylabel('Time (s)')\n",
        "# plt.title('Computation Time vs. Sequence Length')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "DbBEzLVtye_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "sequence_lengths_array = np.array(sequence_lengths).reshape(-1, 1)\n",
        "times_array = np.array(times)\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(sequence_lengths_array, times_array)\n",
        "\n",
        "# Get the fitted line\n",
        "fitted_times = model.predict(sequence_lengths_array)\n",
        "\n",
        "# Calculate R-squared to evaluate the fit\n",
        "r_squared = model.score(sequence_lengths_array, times_array)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sequence_lengths, times, marker='o', linestyle='-', label='Measured Time')\n",
        "plt.plot(sequence_lengths, fitted_times, color='red', linestyle='--', label='Fitted Linear Trend')\n",
        "plt.xlabel('Sequence Length')\n",
        "plt.ylabel('Time (seconds)')\n",
        "plt.title(f'Time Complexity of LUNA Transformer (Linear Fit, R^2 = {r_squared:.4f})')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xAXDbIzHDslL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
